{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oZW0gaQO81Sq"
   },
   "source": [
    "# Transfer learning on the Caltech101 dataset\n",
    "\n",
    "In this notebook, we will consider a more complex dataset than MNIST or CIFAR10. The images in Caltech101 are RGB images (3 channels) with variable size. There are 101 different classes. We will try a very common practice in computer vision nowadays: transfer learning from a pre-trained ImageNet model. \n",
    "\n",
    "Roadmap:\n",
    "- Modify the network from the previous exercise (CIFAR-10) to work with 224x224 images.\n",
    "- Train the model for a while on Caltech101 and see how far we can get.\n",
    "- Take a ResNet34 that was pre-trained on ImageNet-1k and fine-tune it to Caltech101.\n",
    "  - Consider both training only the head (the linear classifier at the end of the network) or the entire network.\n",
    "  - We should be able to reach better performance than our original network in fewer training steps.\n",
    "- Optional: play around with other pre-trained models from `timm` (see info [here](https://github.com/rwightman/pytorch-image-models)).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "htyg7xxN81St"
   },
   "source": [
    "## Preliminaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "v3u2GIWr81Su",
    "outputId": "4bbb197c-5bf8-4f82-f74d-28fcddc175be"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting timm\n",
      "  Obtaining dependency information for timm from https://files.pythonhosted.org/packages/e7/0e/ef97f6d8c399bf5842af0dd5a4f5ac55b2f169d62e29ecbf7663e1cb1438/timm-1.0.9-py3-none-any.whl.metadata\n",
      "  Downloading timm-1.0.9-py3-none-any.whl.metadata (42 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.4/42.4 kB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: torch in /Users/billhikari/anaconda3/lib/python3.11/site-packages (from timm) (2.0.1)\n",
      "Requirement already satisfied: torchvision in /Users/billhikari/anaconda3/lib/python3.11/site-packages (from timm) (0.15.2a0)\n",
      "Requirement already satisfied: pyyaml in /Users/billhikari/anaconda3/lib/python3.11/site-packages (from timm) (6.0)\n",
      "Requirement already satisfied: huggingface_hub in /Users/billhikari/anaconda3/lib/python3.11/site-packages (from timm) (0.15.1)\n",
      "Collecting safetensors (from timm)\n",
      "  Obtaining dependency information for safetensors from https://files.pythonhosted.org/packages/08/8c/ece3bf8756506a890bd980eca02f47f9d98dfbf5ce16eda1368f53560f67/safetensors-0.4.5-cp311-cp311-macosx_11_0_arm64.whl.metadata\n",
      "  Downloading safetensors-0.4.5-cp311-cp311-macosx_11_0_arm64.whl.metadata (3.8 kB)\n",
      "Requirement already satisfied: filelock in /Users/billhikari/anaconda3/lib/python3.11/site-packages (from huggingface_hub->timm) (3.9.0)\n",
      "Requirement already satisfied: fsspec in /Users/billhikari/anaconda3/lib/python3.11/site-packages (from huggingface_hub->timm) (2023.4.0)\n",
      "Requirement already satisfied: requests in /Users/billhikari/anaconda3/lib/python3.11/site-packages (from huggingface_hub->timm) (2.31.0)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /Users/billhikari/anaconda3/lib/python3.11/site-packages (from huggingface_hub->timm) (4.65.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Users/billhikari/anaconda3/lib/python3.11/site-packages (from huggingface_hub->timm) (4.12.2)\n",
      "Requirement already satisfied: packaging>=20.9 in /Users/billhikari/anaconda3/lib/python3.11/site-packages (from huggingface_hub->timm) (23.0)\n",
      "Requirement already satisfied: sympy in /Users/billhikari/anaconda3/lib/python3.11/site-packages (from torch->timm) (1.11.1)\n",
      "Requirement already satisfied: networkx in /Users/billhikari/anaconda3/lib/python3.11/site-packages (from torch->timm) (3.1)\n",
      "Requirement already satisfied: jinja2 in /Users/billhikari/anaconda3/lib/python3.11/site-packages (from torch->timm) (3.1.2)\n",
      "Requirement already satisfied: numpy in /Users/billhikari/anaconda3/lib/python3.11/site-packages (from torchvision->timm) (1.26.4)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /Users/billhikari/anaconda3/lib/python3.11/site-packages (from torchvision->timm) (9.4.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/billhikari/anaconda3/lib/python3.11/site-packages (from jinja2->torch->timm) (2.1.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/billhikari/anaconda3/lib/python3.11/site-packages (from requests->huggingface_hub->timm) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/billhikari/anaconda3/lib/python3.11/site-packages (from requests->huggingface_hub->timm) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/billhikari/anaconda3/lib/python3.11/site-packages (from requests->huggingface_hub->timm) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/billhikari/anaconda3/lib/python3.11/site-packages (from requests->huggingface_hub->timm) (2024.7.4)\n",
      "Requirement already satisfied: mpmath>=0.19 in /Users/billhikari/anaconda3/lib/python3.11/site-packages (from sympy->torch->timm) (1.3.0)\n",
      "Downloading timm-1.0.9-py3-none-any.whl (2.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.3/2.3 MB\u001b[0m \u001b[31m21.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading safetensors-0.4.5-cp311-cp311-macosx_11_0_arm64.whl (381 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m381.5/381.5 kB\u001b[0m \u001b[31m25.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: safetensors, timm\n",
      "Successfully installed safetensors-0.4.5 timm-1.0.9\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "from typing import List, Optional, Callable, Iterator\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "from torch.utils.data import DataLoader, Dataset, Subset\n",
    "\n",
    "sns.set_style(\"whitegrid\")\n",
    "\n",
    "!pip install timm\n",
    "import timm\n",
    "\n",
    "def accuracy(target, pred):\n",
    "    return accuracy_score(target.detach().cpu().numpy(), pred.detach().cpu().numpy())\n",
    "\n",
    "def show_image(img, title=None):\n",
    "    img = img.detach().cpu()\n",
    "    img = img.permute((1, 2, 0)).numpy()\n",
    "    mean = np.array([0.485, 0.456, 0.406])\n",
    "    std = np.array([0.229, 0.224, 0.225])\n",
    "    img = std * img + mean   # unnormalize\n",
    "    img = np.clip(img, 0, 1)\n",
    "    plt.imshow(img)\n",
    "    plt.gca().tick_params(axis=\"both\", which=\"both\", bottom=False, left=False, labelbottom=False, labelleft=False)\n",
    "    if title is not None:\n",
    "        plt.title(title)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FRJYSCs5BUOf"
   },
   "source": [
    "### Set up dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QZeTujLC81S3",
    "outputId": "b46495bc-cfc3-4f0b-cf5e-db4fc38cb372"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2439it [00:00, 34444132.85it/s]\n",
      "/Users/billhikari/anaconda3/lib/python3.11/site-packages/torchvision/datasets/utils.py:260: UserWarning: We detected some HTML elements in the downloaded file. This most likely means that the download triggered an unhandled API response by GDrive. Please report this to torchvision at https://github.com/pytorch/vision/issues including the response:\n",
      "\n",
      "<!DOCTYPE html><html><head><title>Google Drive - Virus scan warning</title><meta http-equiv=\"content-type\" content=\"text/html; charset=utf-8\"/><style nonce=\"1znuCasrOAnpjCmtBHexag\">.goog-link-button{position:relative;color:#15c;text-decoration:underline;cursor:pointer}.goog-link-button-disabled{color:#ccc;text-decoration:none;cursor:default}body{color:#222;font:normal 13px/1.4 arial,sans-serif;margin:0}.grecaptcha-badge{visibility:hidden}.uc-main{padding-top:50px;text-align:center}#uc-dl-icon{display:inline-block;margin-top:16px;padding-right:1em;vertical-align:top}#uc-text{display:inline-block;max-width:68ex;text-align:left}.uc-error-caption,.uc-warning-caption{color:#222;font-size:16px}#uc-download-link{text-decoration:none}.uc-name-size a{color:#15c;text-decoration:none}.uc-name-size a:visited{color:#61c;text-decoration:none}.uc-name-size a:active{color:#d14836;text-decoration:none}.uc-footer{color:#777;font-size:11px;padding-bottom:5ex;padding-top:5ex;text-align:center}.uc-footer a{color:#15c}.uc-footer a:visited{color:#61c}.uc-footer a:active{color:#d14836}.uc-footer-divider{color:#ccc;width:100%}.goog-inline-block{position:relative;display:-moz-inline-box;display:inline-block}* html .goog-inline-block{display:inline}*:first-child+html .goog-inline-block{display:inline}sentinel{}</style><link rel=\"icon\" href=\"//ssl.gstatic.com/docs/doclist/images/drive_2022q3_32dp.png\"/></head><body><div class=\"uc-main\"><div id=\"uc-dl-icon\" class=\"image-container\"><div class=\"drive-sprite-aux-download-file\"></div></div><div id=\"uc-text\"><p class=\"uc-warning-caption\">Google Drive can't scan this file for viruses.</p><p class=\"uc-warning-subcaption\"><span class=\"uc-name-size\"><a href=\"/open?id=137RyRjvTBkBiIfeYBNZBtViDHQ6_Ewsp\">101_ObjectCategories.tar.gz</a> (126M)</span> is too large for Google to scan for viruses. Would you still like to download this file?</p><form id=\"download-form\" action=\"https://drive.usercontent.google.com/download\" method=\"get\"><input type=\"submit\" id=\"uc-download-link\" class=\"goog-inline-block jfk-button jfk-button-action\" value=\"Download anyway\"/><input type=\"hidden\" name=\"id\" value=\"137RyRjvTBkBiIfeYBNZBtViDHQ6_Ewsp\"><input type=\"hidden\" name=\"export\" value=\"download\"><input type=\"hidden\" name=\"confirm\" value=\"t\"><input type=\"hidden\" name=\"uuid\" value=\"1bc19007-e883-4fbc-92b8-b33ba9ce7d1e\"></form></div></div><div class=\"uc-footer\"><hr class=\"uc-footer-divider\"></div></body></html>\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "The MD5 checksum of the download file ./data/caltech101/101_ObjectCategories.tar.gz does not match the one on record.Please delete the file and try again. If the issue persists, please report this to torchvision at https://github.com/pytorch/vision/issues.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 20\u001b[0m\n\u001b[1;32m     17\u001b[0m batch_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m64\u001b[39m  \u001b[38;5;66;03m# both for training and testing\u001b[39;00m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;66;03m# Load dataset (downloaded automatically if missing).\u001b[39;00m\n\u001b[0;32m---> 20\u001b[0m dataset \u001b[38;5;241m=\u001b[39m torchvision\u001b[38;5;241m.\u001b[39mdatasets\u001b[38;5;241m.\u001b[39mCaltech101(root\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m./data\u001b[39m\u001b[38;5;124m'\u001b[39m, download\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtranslate_label\u001b[39m(y, keep_classes):\n\u001b[1;32m     24\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torchvision/datasets/caltech.py:50\u001b[0m, in \u001b[0;36mCaltech101.__init__\u001b[0;34m(self, root, target_type, transform, target_transform, download)\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_type \u001b[38;5;241m=\u001b[39m [verify_str_arg(t, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtarget_type\u001b[39m\u001b[38;5;124m\"\u001b[39m, (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcategory\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mannotation\u001b[39m\u001b[38;5;124m\"\u001b[39m)) \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m target_type]\n\u001b[1;32m     49\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m download:\n\u001b[0;32m---> 50\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdownload()\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_integrity():\n\u001b[1;32m     53\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset not found or corrupted. You can use download=True to download it\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torchvision/datasets/caltech.py:131\u001b[0m, in \u001b[0;36mCaltech101.download\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    128\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFiles already downloaded and verified\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    129\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[0;32m--> 131\u001b[0m download_and_extract_archive(\n\u001b[1;32m    132\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://drive.google.com/file/d/137RyRjvTBkBiIfeYBNZBtViDHQ6_Ewsp\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    133\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mroot,\n\u001b[1;32m    134\u001b[0m     filename\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m101_ObjectCategories.tar.gz\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    135\u001b[0m     md5\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb224c7392d521a49829488ab0f1120d9\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    136\u001b[0m )\n\u001b[1;32m    137\u001b[0m download_and_extract_archive(\n\u001b[1;32m    138\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://drive.google.com/file/d/175kQy3UsZ0wUEHZjqkUDdNVssr7bgh_m\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    139\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mroot,\n\u001b[1;32m    140\u001b[0m     filename\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAnnotations.tar\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    141\u001b[0m     md5\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m6f83eeb1f24d99cab4eb377263132c91\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    142\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torchvision/datasets/utils.py:434\u001b[0m, in \u001b[0;36mdownload_and_extract_archive\u001b[0;34m(url, download_root, extract_root, filename, md5, remove_finished)\u001b[0m\n\u001b[1;32m    431\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m filename:\n\u001b[1;32m    432\u001b[0m     filename \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mbasename(url)\n\u001b[0;32m--> 434\u001b[0m download_url(url, download_root, filename, md5)\n\u001b[1;32m    436\u001b[0m archive \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(download_root, filename)\n\u001b[1;32m    437\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExtracting \u001b[39m\u001b[38;5;132;01m{\u001b[39;00marchive\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m to \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mextract_root\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torchvision/datasets/utils.py:139\u001b[0m, in \u001b[0;36mdownload_url\u001b[0;34m(url, root, filename, md5, max_redirect_hops)\u001b[0m\n\u001b[1;32m    137\u001b[0m file_id \u001b[38;5;241m=\u001b[39m _get_google_drive_file_id(url)\n\u001b[1;32m    138\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m file_id \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 139\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m download_file_from_google_drive(file_id, root, filename, md5)\n\u001b[1;32m    141\u001b[0m \u001b[38;5;66;03m# download the file\u001b[39;00m\n\u001b[1;32m    142\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torchvision/datasets/utils.py:268\u001b[0m, in \u001b[0;36mdownload_file_from_google_drive\u001b[0;34m(file_id, root, filename, md5)\u001b[0m\n\u001b[1;32m    260\u001b[0m             warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m    261\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWe detected some HTML elements in the downloaded file. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    262\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThis most likely means that the download triggered an unhandled API response by GDrive. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    263\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease report this to torchvision at https://github.com/pytorch/vision/issues including \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    264\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mthe response:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mtext\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    265\u001b[0m             )\n\u001b[1;32m    267\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m md5 \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m check_md5(fpath, md5):\n\u001b[0;32m--> 268\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    269\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe MD5 checksum of the download file \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfpath\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m does not match the one on record.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    270\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease delete the file and try again. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    271\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIf the issue persists, please report this to torchvision at https://github.com/pytorch/vision/issues.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    272\u001b[0m     )\n",
      "\u001b[0;31mRuntimeError\u001b[0m: The MD5 checksum of the download file ./data/caltech101/101_ObjectCategories.tar.gz does not match the one on record.Please delete the file and try again. If the issue persists, please report this to torchvision at https://github.com/pytorch/vision/issues."
     ]
    }
   ],
   "source": [
    "imagenet_mean = [0.485, 0.456, 0.406]\n",
    "imagenet_std = [0.229, 0.224, 0.225]\n",
    "default_imagenet_normalization = transforms.Normalize(mean=imagenet_mean, std=imagenet_std)\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.RandomResizedCrop(224, scale=(0.25, 1)),  # crop of random size and aspect ratio, resize to square\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    default_imagenet_normalization,\n",
    "])\n",
    "test_transform = transforms.Compose([\n",
    "    transforms.Resize(224),  # keep aspect ratio\n",
    "    transforms.CenterCrop(224),  # square crop\n",
    "    transforms.ToTensor(),\n",
    "    default_imagenet_normalization,\n",
    "])\n",
    "\n",
    "batch_size = 64  # both for training and testing\n",
    "\n",
    "# Load dataset (downloaded automatically if missing).\n",
    "dataset = torchvision.datasets.Caltech101(root='./data', download=True)\n",
    "\n",
    "\n",
    "def translate_label(y, keep_classes):\n",
    "    try:\n",
    "        return keep_classes.index(y)\n",
    "    except ValueError:\n",
    "        return -1\n",
    "\n",
    "\n",
    "# Filter images such that no class has more than 100 examples.\n",
    "# Loop through dataset in random order, include each image (its index) only if there are \n",
    "# fewer than 100 images of the same class.\n",
    "# Here we also remove the classes \"Faces\" and \"Faces_easy\".\n",
    "skip_classes = [0, 1]   # \"Faces\" and \"Faces_easy\"\n",
    "keep_classes = [c for c in range(len(dataset.categories)) if c not in skip_classes]\n",
    "indices = list(range(len(dataset)))\n",
    "random.shuffle(indices)\n",
    "new_indices = []\n",
    "current_class_size = {class_number: 0 for class_number in np.unique(dataset.y)}\n",
    "for i in indices:\n",
    "    class_number = dataset.y[i]\n",
    "    if class_number not in skip_classes and current_class_size[class_number] < 100:\n",
    "        new_indices.append(i)\n",
    "        current_class_size[class_number] += 1\n",
    "indices = new_indices\n",
    "\n",
    "# Compute subset of labels given new indices selection\n",
    "labels = [translate_label(dataset.y[i], keep_classes) for i in indices]\n",
    "assert max(labels) == 98\n",
    "\n",
    "test_size = 640\n",
    "train_size = len(indices) - test_size\n",
    "train_idx, test_idx = train_test_split(\n",
    "    indices,\n",
    "    train_size=train_size,\n",
    "    test_size=test_size,\n",
    "    shuffle=True,\n",
    "    random_state=42,  # always get the same split\n",
    "    stratify=labels,\n",
    ")\n",
    "\n",
    "\n",
    "class DatasetSubsetWithTransform(Subset):\n",
    "    \n",
    "    def __init__(self, dataset, indices, transform=None):\n",
    "        super().__init__(dataset, indices)\n",
    "        self.transform = transform\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        x, y = super().__getitem__(idx)\n",
    "        # Convert PIL image to RGB (some of them are greyscale)\n",
    "        x = x.convert('RGB')\n",
    "        if self.transform is not None:\n",
    "            x = self.transform(x)\n",
    "        y = translate_label(y, keep_classes)\n",
    "        return x, y\n",
    "    \n",
    "\n",
    "train_set = DatasetSubsetWithTransform(dataset, train_idx, train_transform)\n",
    "test_set = DatasetSubsetWithTransform(dataset, test_idx, test_transform)\n",
    "train_loader = DataLoader(train_set, batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "test_loader = DataLoader(test_set, batch_size=batch_size, shuffle=False, drop_last=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hnIEkOhCBa5k"
   },
   "source": [
    "### A closer look at the dataset\n",
    "\n",
    "We first plot the size of each class and observe the class distribution is not uniform.\n",
    "\n",
    "Then we show random examples from the dataset, annotated with the class label and index.\n",
    "Note that the training images include standard augmentations typically used for vision models (defined above)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "MV8Th7V6eYx0",
    "outputId": "26bdc902-aa19-41b1-a2ad-54174c1095d1"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'labels' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m label_idxs, counts \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39munique(labels, return_counts\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m      2\u001b[0m plt\u001b[38;5;241m.\u001b[39mfigure(figsize\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m20\u001b[39m, \u001b[38;5;241m4.2\u001b[39m))\n\u001b[1;32m      3\u001b[0m sns\u001b[38;5;241m.\u001b[39mbarplot(x\u001b[38;5;241m=\u001b[39m[dataset\u001b[38;5;241m.\u001b[39mcategories[keep_classes[label]] \u001b[38;5;28;01mfor\u001b[39;00m label \u001b[38;5;129;01min\u001b[39;00m label_idxs], y\u001b[38;5;241m=\u001b[39mcounts, color\u001b[38;5;241m=\u001b[39msns\u001b[38;5;241m.\u001b[39mcolor_palette(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmuted\u001b[39m\u001b[38;5;124m'\u001b[39m)[\u001b[38;5;241m0\u001b[39m])\n",
      "\u001b[0;31mNameError\u001b[0m: name 'labels' is not defined"
     ]
    }
   ],
   "source": [
    "label_idxs, counts = np.unique(labels, return_counts=True)\n",
    "plt.figure(figsize=(20, 4.2))\n",
    "sns.barplot(x=[dataset.categories[keep_classes[label]] for label in label_idxs], y=counts, color=sns.color_palette('muted')[0])\n",
    "plt.xticks(rotation=90)\n",
    "plt.xlabel(\"Class\")\n",
    "plt.ylabel(\"Number of examples\")\n",
    "plt.title(\"Class distribution\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "def show_dataset_examples(dataloader):\n",
    "    images, labels = next(iter(dataloader))\n",
    "    with sns.axes_style(\"white\"):\n",
    "      fig, axes = plt.subplots(4, 8, figsize=(16, 9.5))\n",
    "    axes = [ax for axes_ in axes for ax in axes_]   # flatten\n",
    "    for j, (img, label) in enumerate(zip(images[:32], labels[:32])):\n",
    "        plt.sca(axes[j])\n",
    "        show_image(img, title=f\"{dataset.categories[keep_classes[label.item()]]} ({label.item()})\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "print(\"\\n\\nTrain images (including augmentations):\")\n",
    "show_dataset_examples(train_loader)\n",
    "print(\"\\n\\nTest images:\")\n",
    "show_dataset_examples(test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Wt3BVFMF81TI"
   },
   "source": [
    "## Define a neural network\n",
    "\n",
    "**Assignment 1:** Adapt the CNN from the previous lab (CIFAR-10) to handle 224x224 images. We recommend reducing significantly the size of the tensors before flattening, by adding either convolutional layers with stride>1 or [MaxPool2d](https://pytorch.org/docs/stable/generated/torch.nn.MaxPool2d.html) layers (but see e.g. also [AvgPool2d](https://pytorch.org/docs/stable/generated/torch.nn.AvgPool2d.html))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6VbF_R2IMrDT",
    "outputId": "62ddf5f5-2408-4aef-cc03-b0e95a11af36"
   },
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "\n",
    "    def __init__(self, num_classes):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.net = ...   # Your code here!\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "    \n",
    "    \n",
    "model = Model(num_classes=len(np.unique(labels)))\n",
    "device = torch.device('cuda')  # use cuda or cpu\n",
    "model = model.to(device)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7-IUg3sq81TQ"
   },
   "source": [
    "## Define loss function and optimizer\n",
    "\n",
    "**Assignment 2:** Implement the criterion and optimizer, as in the previous notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "48AX85QP81TR"
   },
   "outputs": [],
   "source": [
    "loss_fn = None  # Your code here!\n",
    "optimizer = None  # Your code here!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-WneIN7C81TV"
   },
   "source": [
    "## Train the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "iWT0ULctWvm1",
    "outputId": "632ea54a-8070-4310-bcaa-f1b9b0f354c4"
   },
   "outputs": [],
   "source": [
    "# Test the forward pass with dummy data\n",
    "out = model(torch.randn(2, 3, 224, 224).to(device))\n",
    "print(\"Output shape:\", out.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 451
    },
    "id": "NkUanRRb81TW",
    "outputId": "f024acc3-a195-44e4-b4d9-9c512f6c05f7"
   },
   "outputs": [],
   "source": [
    "num_epochs = 10\n",
    "validation_every_steps = 50\n",
    "\n",
    "step = 0\n",
    "model.train()\n",
    "\n",
    "train_accuracies = []\n",
    "valid_accuracies = []\n",
    "        \n",
    "for epoch in range(num_epochs):\n",
    "    \n",
    "    train_accuracies_batches = []\n",
    "    \n",
    "    for inputs, targets in train_loader:\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "        \n",
    "        # Forward pass.\n",
    "        output = model(inputs)\n",
    "        \n",
    "        # Compute loss.\n",
    "        loss = loss_fn(output, targets)\n",
    "        \n",
    "        # Clean up gradients from the model.\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Compute gradients based on the loss from the current batch (backpropagation).\n",
    "        loss.backward()\n",
    "        \n",
    "        # Take one optimizer step using the gradients computed in the previous step.\n",
    "        optimizer.step()\n",
    "        \n",
    "        step += 1\n",
    "        \n",
    "        # Compute accuracy.\n",
    "        predictions = output.max(1)[1]\n",
    "        train_accuracies_batches.append(accuracy(targets, predictions))\n",
    "        \n",
    "        if step % validation_every_steps == 0:\n",
    "            \n",
    "            # Append average training accuracy to list.\n",
    "            train_accuracies.append(np.mean(train_accuracies_batches))\n",
    "            \n",
    "            train_accuracies_batches = []\n",
    "        \n",
    "            # Compute accuracies on validation set.\n",
    "            valid_accuracies_batches = []\n",
    "            with torch.no_grad():\n",
    "                model.eval()\n",
    "                for inputs, targets in test_loader:\n",
    "                    inputs, targets = inputs.to(device), targets.to(device)\n",
    "                    output = model(inputs)\n",
    "                    loss = loss_fn(output, targets)\n",
    "\n",
    "                    predictions = output.max(1)[1]\n",
    "\n",
    "                    # Multiply by len(x) because the final batch of DataLoader may be smaller (drop_last=False).\n",
    "                    valid_accuracies_batches.append(accuracy(targets, predictions) * len(inputs))\n",
    "\n",
    "                model.train()\n",
    "                \n",
    "            # Append average validation accuracy to list.\n",
    "            valid_accuracies.append(np.sum(valid_accuracies_batches) / len(test_set))\n",
    "     \n",
    "            print(f\"Step {step:<5}   training accuracy: {train_accuracies[-1]}\")\n",
    "            print(f\"             test accuracy: {valid_accuracies[-1]}\")\n",
    "\n",
    "print(\"Finished training.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0qAsbC8I81Ta"
   },
   "source": [
    "## Test the network on the test data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Smv6_BwF81Ti",
    "outputId": "a65a986a-3497-4aa4-dd5f-3bedc949e84d"
   },
   "outputs": [],
   "source": [
    "# Evaluate test set\n",
    "with torch.no_grad():\n",
    "    model.eval()\n",
    "    test_accuracies = []\n",
    "    for inputs, targets in test_loader:\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "        output = model(inputs)\n",
    "        loss = loss_fn(output, targets)\n",
    "\n",
    "        predictions = output.max(1)[1]\n",
    "\n",
    "        # Multiply by len(x) because the final batch of DataLoader may be smaller (drop_last=True).\n",
    "        test_accuracies.append(accuracy(targets, predictions) * len(inputs))\n",
    "\n",
    "    test_accuracy = np.sum(test_accuracies) / len(test_set)\n",
    "    print(f\"Test accuracy: {test_accuracy:.3f}\")\n",
    "    \n",
    "    model.train()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tbn2kNxCRFoc"
   },
   "source": [
    "## Using a pre-trained model\n",
    "\n",
    "Here we will load a ResNet34 that was pre-trained on ImageNet. We then discard the linear classifier at the end of the network (the \"head\" of the network) and replace it with a new one that outputs the desired number of logits for classification. To get a rough idea of the structure of the model, we print it below.\n",
    "\n",
    "The argument `finetune_entire_model` in `initialize_model()` controls whether the entire pre-trained model is fine-tuned. When this is `False`, only the linear head is trained, and the rest of the model is fixed. The idea is that the features extracted by the ImageNet model, up to the final classification layer, are very informative also on other datasets (see, e.g., [this paper](https://arxiv.org/abs/1910.04867) on the transferability of deep representations in large vision models).\n",
    "\n",
    "We will start here by training only the linear head. You can experiment different models and variations.\n",
    "\n",
    "Below, we define the model and forget the one we just trained. After that, you can go back to the section \"Define loss function and optimizer\" and re-execute the notebook from there, to train and evaluate the new model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "o4W_fQASeYx1",
    "outputId": "40878ae3-c3fa-41d3-af75-43f31822f115"
   },
   "outputs": [],
   "source": [
    "def initialize_model(model_name: str, *, num_classes: int, finetune_entire_model: bool = False):\n",
    "    \"\"\"Returns a pretrained model with a new last layer, and a dict with additional info.\n",
    "\n",
    "    The dict now contains the number of model parameters, computed as the number of\n",
    "    trainable parameters as soon as the model is loaded.\n",
    "    \"\"\"\n",
    "\n",
    "    print(\n",
    "        f\"Loading model '{model_name}', with \"\n",
    "        f\"finetune_entire_model={finetune_entire_model}, changing the \"\n",
    "        f\"last layer to output {num_classes} logits.\"\n",
    "    )\n",
    "    model = timm.create_model(\n",
    "        model_name, pretrained=True, num_classes=num_classes\n",
    "    )\n",
    "\n",
    "    num_model_parameters = 0\n",
    "    for p in model.parameters():\n",
    "        if p.requires_grad:\n",
    "            num_model_parameters += p.numel()\n",
    "\n",
    "    if not finetune_entire_model:\n",
    "        for name, param in model.named_parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "        # Layer names are not consistent, so we have to consider a few cases. This might \n",
    "        # break for arbitrary models from timm (we have not checked all available models).\n",
    "        layer = None\n",
    "        if hasattr(model, \"classifier\"):\n",
    "            if isinstance(model.classifier, nn.Linear):\n",
    "                layer = model.classifier\n",
    "        elif hasattr(model, \"head\"):\n",
    "            if isinstance(model.head, nn.Linear):\n",
    "                layer = model.head\n",
    "            elif hasattr(model.head, \"fc\") and isinstance(model.head.fc, nn.Linear):\n",
    "                layer = model.head.fc\n",
    "            elif hasattr(model.head, \"l\") and isinstance(model.head.l, nn.Linear):\n",
    "                layer = model.head.l\n",
    "        elif hasattr(model, \"fc\"):\n",
    "            if isinstance(model.fc, nn.Linear):\n",
    "                layer = model.fc\n",
    "        if layer is None:\n",
    "            raise ValueError(f\"Couldn't automatically find last layer of model.\")\n",
    "        \n",
    "        # Make the last layer trainable.\n",
    "        layer.weight.requires_grad_()\n",
    "        layer.bias.requires_grad_()\n",
    "\n",
    "    num_trainable_model_parameters = 0\n",
    "    for p in model.parameters():\n",
    "        if p.requires_grad:\n",
    "            num_trainable_model_parameters += p.numel()\n",
    "\n",
    "    return model, {\n",
    "        \"num_model_parameters\": num_model_parameters,\n",
    "        \"num_trainable_model_parameters\": num_trainable_model_parameters,\n",
    "    }\n",
    "\n",
    "model, data = initialize_model('resnet34d', num_classes=len(np.unique(labels)), finetune_entire_model=False)\n",
    "\n",
    "print(model)\n",
    "print(\"Number of model parameters:\", data[\"num_model_parameters\"])\n",
    "print(\"Number of trainable parameters:\", data[\"num_trainable_model_parameters\"])\n",
    "\n",
    "device = torch.device('cuda')  # use cuda or cpu\n",
    "model = model.to(device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ocnQOBAl81Tn"
   },
   "source": [
    "**Assignment 3:** \n",
    "\n",
    "1. Train the linear classifier on top of the pre-trained network, and observe how quickly you can get pretty good results, compared to training a smaller network from scratch as above.\n",
    "\n",
    "2. Go back and change argument to finetune entire network, maybe adjust learning rate, see if you can get better performance than before and if you run into any issues.\n",
    "\n",
    "3. Optional: experiment with `timm`: try smaller or larger models, including state-of-the-art models, e.g. based on vision transformers (ViT) or MLP-Mixers.\n",
    "\n",
    "4. Briefly describe what you did and any experiments you did along the way as well as what results you obtained.\n",
    "Did anything surprise you during the exercise?\n",
    "\n",
    "5. Write down key lessons/insights you got during this exercise.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "4.3-CNN-transfer.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
